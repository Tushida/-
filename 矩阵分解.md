# 矩阵分解算法

## 原理

在矩阵分解的算法框架下， **可以通过分解协同过滤的共现矩阵（评分矩阵）来得到用户和物品的隐向量**，原理如下：

![](G:\工作笔记\推荐系统\新闻推荐系统\图像\矩阵分解.png)

\+ 矩阵分解算法将 $m\times n$ 维的共享矩阵 $R$ ，分解成 $m \times k$ 维的用户矩阵 $U$ 和 $k \times n$ 维的物品矩阵 $V$ 相乘的形式。 + 其中，$m$ 是用户数量， $n$ 是物品数量， $k$ 是隐向量维度， 也就是隐含特征个数。 + 这里的隐含特征没有太好的可解释性，需要模型自己去学习。 + 一般而言， $k$ 越大隐向量能承载的信息内容越多，表达能力也会更强，但相应的学习难度也会增加。所以，我们需要根据训练集样本的数量去选择合适的数值，在保证信息学习相对完整的前提下，降低模型的学习难度。

## 评分预测

在分解得到用户矩阵和物品矩阵后，若要计算用户 u 对物品 i 的评分，公式如下： $$ \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{k=1}^{K} p_{u, k} q_{i,k} $$

- 其中，向量 pu 表示用户 u 的隐向量，向量 qi 表示物品 i 的隐向量。
- 用户向量和物品向量的内积 puTqi 可以表示为用户 u 对物品 i 的预测评分。
- pu,k 和 qi,k 是模型的参数， pu,k 度量的是用户 u 的兴趣和第 k 个隐类的关系，$q_{i,k}$ 度量了第 k 个隐类和物品 i 之间的关系。

## 矩阵分解求解

常用的矩阵分解方法有特征值分解(EVD)或者奇异值分解(SVD）， 具体原理可参考：

> [奇异值分解svd原理详解及推导](https://blog.csdn.net/wuzhongqiang/article/details/108168238)

- 对于 EVD， 它要求分解的矩阵是方阵， 绝大部分场景下用户-物品矩阵不满足这个要求。
- 传统的 SVD 分解， 会要求原始矩阵是稠密的。但现实中用户的评分矩阵是非常稀疏的。
  - 如果想用奇异值分解， 就必须对缺失的元素进行填充（比如填 0 ）。
  - 填充不但会导致空间复杂度增高，且补全内容不一定准确。
  - 另外，SVD 分解计算复杂度非常高，而用户-物品的评分矩阵较大，不具备普适性。

## 代码实现

```python
import random
import math


class BiasSVD():
    def __init__(self, rating_data, F=5, alpha=0.1, lmbda=0.1, max_iter=100):
        self.F = F          # 这个表示隐向量的维度
        self.P = dict()     # 用户矩阵P  大小是[users_num, F]
        self.Q = dict()     # 物品矩阵Q  大小是[item_nums, F]
        self.bu = dict()    # 用户偏置系数
        self.bi = dict()    # 物品偏置系数
        self.mu = 0         # 全局偏置系数
        self.alpha = alpha  # 学习率
        self.lmbda = lmbda  # 正则项系数
        self.max_iter = max_iter        # 最大迭代次数
        self.rating_data = rating_data  # 评分矩阵

        for user, items in self.rating_data.items():
            # 初始化矩阵P和Q, 随机数需要和1/sqrt(F)成正比
            self.P[user] = [random.random() / math.sqrt(self.F) for x in range(0, F)]
            self.bu[user] = 0
            for item, rating in items.items():
                if item not in self.Q:
                    self.Q[item] = [random.random() / math.sqrt(self.F) for x in range(0, F)]
                    self.bi[item] = 0

    # 采用随机梯度下降的方式训练模型参数
    def train(self):
        cnt, mu_sum = 0, 0
        for user, items in self.rating_data.items():
            for item, rui in items.items():
                mu_sum, cnt = mu_sum + rui, cnt + 1
        self.mu = mu_sum / cnt

        for step in range(self.max_iter):
            # 遍历所有的用户及历史交互物品
            for user, items in self.rating_data.items():
                # 遍历历史交互物品
                for item, rui in items.items():
                    rhat_ui = self.predict(user, item)  # 评分预测
                    e_ui = rui - rhat_ui  				# 评分预测偏差

                    # 参数更新
                    self.bu[user] += self.alpha * (e_ui - self.lmbda * self.bu[user])
                    self.bi[item] += self.alpha * (e_ui - self.lmbda * self.bi[item])
                    for k in range(0, self.F):
                        self.P[user][k] += self.alpha * (e_ui * self.Q[item][k] - self.lmbda * self.P[user][k])
                        self.Q[item][k] += self.alpha * (e_ui * self.P[user][k] - self.lmbda * self.Q[item][k])
            # 逐步降低学习率
            self.alpha *= 0.1


    # 评分预测
    def predict(self, user, item):
        return sum(self.P[user][f] * self.Q[item][f] for f in range(0, self.F)) + self.bu[user] + self.bi[
            item] + self.mu


# 通过字典初始化训练样本，分别表示不同用户（1-5）对不同物品（A-E)的真实评分
def loadData():
    rating_data={1: {'A': 5, 'B': 3, 'C': 4, 'D': 4},
           2: {'A': 3, 'B': 1, 'C': 2, 'D': 3, 'E': 3},
           3: {'A': 4, 'B': 3, 'C': 4, 'D': 3, 'E': 5},
           4: {'A': 3, 'B': 3, 'C': 1, 'D': 5, 'E': 4},
           5: {'A': 1, 'B': 5, 'C': 5, 'D': 2, 'E': 1}
          }
    return rating_data

# 加载数据
rating_data = loadData()
# 建立模型
basicsvd = BiasSVD(rating_data, F=10)
# 参数训练
basicsvd.train()
# 预测用户1对物品E的评分
for item in ['E']:
    print(item, basicsvd.predict(1, item))

# 预测结果：E 3.685084274454321
```

