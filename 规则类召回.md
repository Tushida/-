在新闻推荐系统中，采用了多路召回的形式：

## 规则类召回：

规则类召回主要是针对具体业务场景设计的，这里的规则类召回主要包含了热门召回、地域召回

热门召回：热门页的内容通过热门召回来实现

​	实现方法：统计所有物料的热度分数，该热度分数由一个加权的分数来表示，最后根据热度分数倒排；也可以考虑个性化的用户热门列表，通过用户的一些兴趣标签；

地域召回：

​	实现方法：首先通过地域对物料进行分类，对于不同地域中的物料进行召回；

```python
 def province_recall(self, N, cur_time):
        article_df = self.get_article_stat_data(self.train_log_df, self.doc_info_df)
        region_articles_df = pd.merge(self.train_log_df, self.user_info_df, how='left', on='user_id')
        region_articles_df = region_articles_df[['user_id', 'article_id', 'province', 'city']]
        region_articles_df = pd.merge(region_articles_df, article_df, how='left', on='article_id')
        # 去除时间为空, 以及一些异常数据
        region_articles_df['ctime'] = region_articles_df['ctime'].astype(str)
        region_articles_df = region_articles_df[region_articles_df['ctime'].str.isnumeric()]
        # 分组
        province_df_dict = {}
        for name, df in region_articles_df.groupby('province'):
            if df.shape[0] < 5000:
                continue
            # 分完组之后可以取出重复数据
            df = df[['article_id', 'expo_num', 'click_num', 'click_rate', 'ctime', 'cate']].\
                drop_duplicates(subset='article_id').reset_index(drop=True)
            province_df_dict[name] = df
```

上述代码为根据地域进行物料分类，并在分类完成的物料中筛选出较为优质的一部分进行展示；

### 基于YouTubeDNN的召回策略：

​	1.首先：建立索引-原始id字典（索引：embedding）字典 

​	2.对类别特征进行label Encoder

```python
base_features = ['user_id', 'article_id', 'city', 'age', 'gender']
    feature_max_idx = {}
    for f in base_features:
        lbe = LabelEncoder()
        data[f] = lbe.fit_transform(data[f])
        feature_max_idx[f] = data[f].max() + 1
```

​	3.保存每篇文章的点击数量，后面会对高热文章实施打压

​	这里的步骤主要包括：构造数据集->训练模型->产生召回结果：

​	构造数据集：

​	首先将数据按照时间戳进行排序，然后按照用户进行分组，对于每个用户的点击，采用**滑动窗口**的形式，边滑动边构造样本，在生成一条正样本序列（对于长度异常的序列，要进行截断或者丢弃）的同时，也要加入一定比例的负样本；这里加入了热门打压的策略（降低成为正样本的概率）：

```python
for i in range(1, len(pos_list)):
                hist = pos_list[:i]
                # 这里采用打压热门item策略，降低高展item成为正样本的概率
                freq_i = doc_clicked_count_dict[pos_list[i]] / (np.sum(list(doc_clicked_count_dict.values())))
                p_posi = (np.sqrt(freq_i/0.001)+1)*(0.001/freq_i)
                
                # p_posi=0.3  表示该item_i成为正样本的概率是0.3，
                if user_control_flag and i != len(pos_list) - 1:
                    if random.random() > (1-p_posi):
                        row = [user_id, hist[::-1], pos_list[i], hist_click.iloc[0]['city'], hist_click.iloc[0]['age'], hist_click.iloc[0]['gender'], hist_click.iloc[i]['example_age'], 1, len(hist[::-1])]
                        train_set.append(row)
                        
                        for negi in range(negsample):
                            row = [user_id, hist[::-1], neg_list[i*negsample+negi], hist_click.iloc[0]['city'], hist_click.iloc[0]['age'], hist_click.iloc[0]['gender'], hist_click.iloc[i]['example_age'], 0, len(hist[::-1])]
                            train_set.append(row)
```

​	构造模型输入：

​	这一步的目的是将数据集中的数据转为字典的形式，让模型知道每个特征对应的含义

```python
def gen_model_input(train_set, his_seq_max_len):
    """构造模型的输入"""
    # row: [user_id, hist_list, cur_doc_id, city, age, gender, label, hist_len]
    train_uid = np.array([row[0] for row in train_set])
    train_hist_seq = [row[1] for row in train_set]
    train_iid = np.array([row[2] for row in train_set])
    train_u_city = np.array([row[3] for row in train_set])
    train_u_age = np.array([row[4] for row in train_set])
    train_u_gender = np.array([row[5] for row in train_set])
    train_u_example_age = np.array([row[6] for row in train_set])
    train_label = np.array([row[7] for row in train_set])
    train_hist_len = np.array([row[8] for row in train_set])
    
    train_seq_pad = pad_sequences(train_hist_seq, maxlen=his_seq_max_len, padding='post', truncating='post', value=0)
    train_model_input = {
        "user_id": train_uid,
        "click_doc_id": train_iid,
        "hist_doc_ids": train_seq_pad,
        "hist_len": train_hist_len,
        "u_city": train_u_city,
        "u_age": train_u_age,
        "u_gender": train_u_gender, 
        "u_example_age":train_u_example_age
    }
    return train_model_input, train_label
```

接下来我们进行模型的训练：

​	首先，进行特征封装：标记离散特征和连续特征：

```python
  user_feature_columns = [
        SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),
        VarLenSparseFeat(SparseFeat('hist_doc_ids', feature_max_idx['article_id'], embedding_dim,
                                                        embedding_name="click_doc_id"), his_seq_maxlen, 'mean', 'hist_len'),    
        
        SparseFeat('u_city', feature_max_idx['city'], embedding_dim),
        SparseFeat('u_age', feature_max_idx['age'], embedding_dim),
        SparseFeat('u_gender', feature_max_idx['gender'], embedding_dim),
        DenseFeat('u_example_age', 1,)
    ]
    doc_feature_columns = [
        SparseFeat('click_doc_id', feature_max_idx['article_id'], embedding_dim)
        # 这里后面也可以把文章的类别画像特征加入
    ]
```

然后可以进行模型训练：

```python
# 定义模型
    model = YoutubeDNN(user_feature_columns, doc_feature_columns, num_sampled=5, user_dnn_hidden_units=(64, embedding_dim))
    
```

模型训练完之后，就能从模型里面拿用户向量和item向量：

```python
 user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)
    doc_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)
    
    # 保存当前的item_embedding 和 user_embedding 排序的时候可能能够用到，但是需要注意保存的时候需要和原始的id对应
    user_embs = user_embedding_model.predict(test_model_input, batch_size=2 ** 12)
    doc_embs = doc_embedding_model.predict(doc_model_input, batch_size=2 ** 12)
```

然后我们进行向量最近邻检索，为用户召回相似Item；

